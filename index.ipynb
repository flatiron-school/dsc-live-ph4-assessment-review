{"cells": [{"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["# Phase 4 Code Challenge Review"]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["TOC:\n", "\n", "  - [PCA](#pca)\n", "  - [NLP](#nlp)\n", "  - [Time Series](#ts)  \n", "  - [Clustering](#clust)\n"]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["<a id='pca'></a>"]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["# PCA"]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["Principal Component Analysis creates a set of features called principal compenents. PCA reduces the dimensions of our data set from the original n number of features to a specified number of components.  \n", "\n", "The components are built successively.  Describe what the first principal component represents in relation to the original feature set."]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["> Your answer here"]}, {"cell_type": "markdown", "metadata": {"index": 6}, "source": ["\n", "The first principal component points in the direction that explains the most variance of the original feature set.  Each principal component is composed of a combination of the original components. A larger weight in the first principal component indicates a larger variance in the original feature set."]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["Why is scaling important for PCA?"]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["> Your answer here"]}, {"cell_type": "markdown", "metadata": {"index": 9}, "source": ["\n", "Scaling is important because variance in a feature measured in a relatively small unit can be just as or more important than a feature measured in a large unit.  In other words, the dependent variable may depend more on the feature with the smaller unit than the large.  When transforming a dataset with PCA, the PCA object finds the direction that explains the most total variance in the feature set.  It will then tend to identify features with larger units as the most important.  By scaling, the unit is taken out of the picture.  PCA will be able to identify features whose original scale is smaller, but whose variation correlates more closely with the dependent feature.  \n", "\n", "Take for example a model that attempts to predict weight of a new born with age of the mother in years and height of the mother in meters.  Without scaling, the total variance of age across all subjects will be much greater than the total variance of height, simply because of the unit.  If one fits a PCA object to height and age, the first principal component will be more heavily influenced by age.  However, the height of the mother likely is a better predictor than age, but because of its relatively small variance in the original unit (meters), its influence is obscured. By scaling, PCA will consider the relative variance of height and age without regard for the unit. \n"]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["What are some reasons for using PCA?\n"]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["> Your answer here"]}, {"cell_type": "markdown", "metadata": {"index": 12}, "source": ["\n", "1. PCA can speed up computation time. \n", "2. PCA can help with overfitting and decrease the overall prediction error of the model.\n", "3. A similar advantage to #2 is that PCA eliminates multicollinearity.  Because each component is built orthogonally to the last, all multicollinearity is eliminated.  \n", "4. PCA can be used for visualization.  Reducing the original data set to two dimensions allows a representation of the data to be plotted on an x-y plane.\n"]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["How can one determine how many principle components to use in a model?"]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["> Your answer here"]}, {"cell_type": "markdown", "metadata": {"index": 15}, "source": ["Each successive principal component explains a different aspect of the variance of the feature set.  After the PCA object has been fit, it has an attribute named explained_variance_ratio, which describes what percentage of the variance is explained by each component.  The first component will have the largest value for explained variance ratio.  And the variance explained will decrease with each successive component. At some point, adding another principal component will result in only a small additional percent of variance explained.  Looking at a graph of the number of components vs. cumulative explained variance ratio, the curve will stop increasing significantly and start to level off.  A good choice for the number of principal components would be the component after which a significant increase of explained variance stops.\n", "\n", "The PCA object can also be given a proportion as the argument p_components.  PCA will stop after that decimal.  If .8 is given, PCA will stop adding components after the previous components have explained 80% of the variance."]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["# Now let's implement PCA in code."]}, {"cell_type": "code", "execution_count": 1, "metadata": {"index": 17}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>mean radius</th>\n", "      <th>mean texture</th>\n", "      <th>mean perimeter</th>\n", "      <th>mean area</th>\n", "      <th>mean smoothness</th>\n", "      <th>mean compactness</th>\n", "      <th>mean concavity</th>\n", "      <th>mean concave points</th>\n", "      <th>mean symmetry</th>\n", "      <th>mean fractal dimension</th>\n", "      <th>...</th>\n", "      <th>worst radius</th>\n", "      <th>worst texture</th>\n", "      <th>worst perimeter</th>\n", "      <th>worst area</th>\n", "      <th>worst smoothness</th>\n", "      <th>worst compactness</th>\n", "      <th>worst concavity</th>\n", "      <th>worst concave points</th>\n", "      <th>worst symmetry</th>\n", "      <th>worst fractal dimension</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>0</th>\n", "      <td>17.99</td>\n", "      <td>10.38</td>\n", "      <td>122.80</td>\n", "      <td>1001.0</td>\n", "      <td>0.11840</td>\n", "      <td>0.27760</td>\n", "      <td>0.3001</td>\n", "      <td>0.14710</td>\n", "      <td>0.2419</td>\n", "      <td>0.07871</td>\n", "      <td>...</td>\n", "      <td>25.38</td>\n", "      <td>17.33</td>\n", "      <td>184.60</td>\n", "      <td>2019.0</td>\n", "      <td>0.1622</td>\n", "      <td>0.6656</td>\n", "      <td>0.7119</td>\n", "      <td>0.2654</td>\n", "      <td>0.4601</td>\n", "      <td>0.11890</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1</th>\n", "      <td>20.57</td>\n", "      <td>17.77</td>\n", "      <td>132.90</td>\n", "      <td>1326.0</td>\n", "      <td>0.08474</td>\n", "      <td>0.07864</td>\n", "      <td>0.0869</td>\n", "      <td>0.07017</td>\n", "      <td>0.1812</td>\n", "      <td>0.05667</td>\n", "      <td>...</td>\n", "      <td>24.99</td>\n", "      <td>23.41</td>\n", "      <td>158.80</td>\n", "      <td>1956.0</td>\n", "      <td>0.1238</td>\n", "      <td>0.1866</td>\n", "      <td>0.2416</td>\n", "      <td>0.1860</td>\n", "      <td>0.2750</td>\n", "      <td>0.08902</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2</th>\n", "      <td>19.69</td>\n", "      <td>21.25</td>\n", "      <td>130.00</td>\n", "      <td>1203.0</td>\n", "      <td>0.10960</td>\n", "      <td>0.15990</td>\n", "      <td>0.1974</td>\n", "      <td>0.12790</td>\n", "      <td>0.2069</td>\n", "      <td>0.05999</td>\n", "      <td>...</td>\n", "      <td>23.57</td>\n", "      <td>25.53</td>\n", "      <td>152.50</td>\n", "      <td>1709.0</td>\n", "      <td>0.1444</td>\n", "      <td>0.4245</td>\n", "      <td>0.4504</td>\n", "      <td>0.2430</td>\n", "      <td>0.3613</td>\n", "      <td>0.08758</td>\n", "    </tr>\n", "    <tr>\n", "      <th>3</th>\n", "      <td>11.42</td>\n", "      <td>20.38</td>\n", "      <td>77.58</td>\n", "      <td>386.1</td>\n", "      <td>0.14250</td>\n", "      <td>0.28390</td>\n", "      <td>0.2414</td>\n", "      <td>0.10520</td>\n", "      <td>0.2597</td>\n", "      <td>0.09744</td>\n", "      <td>...</td>\n", "      <td>14.91</td>\n", "      <td>26.50</td>\n", "      <td>98.87</td>\n", "      <td>567.7</td>\n", "      <td>0.2098</td>\n", "      <td>0.8663</td>\n", "      <td>0.6869</td>\n", "      <td>0.2575</td>\n", "      <td>0.6638</td>\n", "      <td>0.17300</td>\n", "    </tr>\n", "    <tr>\n", "      <th>4</th>\n", "      <td>20.29</td>\n", "      <td>14.34</td>\n", "      <td>135.10</td>\n", "      <td>1297.0</td>\n", "      <td>0.10030</td>\n", "      <td>0.13280</td>\n", "      <td>0.1980</td>\n", "      <td>0.10430</td>\n", "      <td>0.1809</td>\n", "      <td>0.05883</td>\n", "      <td>...</td>\n", "      <td>22.54</td>\n", "      <td>16.67</td>\n", "      <td>152.20</td>\n", "      <td>1575.0</td>\n", "      <td>0.1374</td>\n", "      <td>0.2050</td>\n", "      <td>0.4000</td>\n", "      <td>0.1625</td>\n", "      <td>0.2364</td>\n", "      <td>0.07678</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "<p>5 rows \u00d7 30 columns</p>\n", "</div>"], "text/plain": ["   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n", "0        17.99         10.38          122.80     1001.0          0.11840   \n", "1        20.57         17.77          132.90     1326.0          0.08474   \n", "2        19.69         21.25          130.00     1203.0          0.10960   \n", "3        11.42         20.38           77.58      386.1          0.14250   \n", "4        20.29         14.34          135.10     1297.0          0.10030   \n", "\n", "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n", "0           0.27760          0.3001              0.14710         0.2419   \n", "1           0.07864          0.0869              0.07017         0.1812   \n", "2           0.15990          0.1974              0.12790         0.2069   \n", "3           0.28390          0.2414              0.10520         0.2597   \n", "4           0.13280          0.1980              0.10430         0.1809   \n", "\n", "   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n", "0                 0.07871  ...         25.38          17.33           184.60   \n", "1                 0.05667  ...         24.99          23.41           158.80   \n", "2                 0.05999  ...         23.57          25.53           152.50   \n", "3                 0.09744  ...         14.91          26.50            98.87   \n", "4                 0.05883  ...         22.54          16.67           152.20   \n", "\n", "   worst area  worst smoothness  worst compactness  worst concavity  \\\n", "0      2019.0            0.1622             0.6656           0.7119   \n", "1      1956.0            0.1238             0.1866           0.2416   \n", "2      1709.0            0.1444             0.4245           0.4504   \n", "3       567.7            0.2098             0.8663           0.6869   \n", "4      1575.0            0.1374             0.2050           0.4000   \n", "\n", "   worst concave points  worst symmetry  worst fractal dimension  \n", "0                0.2654          0.4601                  0.11890  \n", "1                0.1860          0.2750                  0.08902  \n", "2                0.2430          0.3613                  0.08758  \n", "3                0.2575          0.6638                  0.17300  \n", "4                0.1625          0.2364                  0.07678  \n", "\n", "[5 rows x 30 columns]"]}, "execution_count": 1, "metadata": {}, "output_type": "execute_result"}], "source": ["import pandas as pd\n", "from sklearn.datasets import  load_breast_cancer\n", "data = load_breast_cancer()\n", "X = pd.DataFrame(data['data'], columns = data['feature_names'])\n", "X.head()"]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": 18}, "outputs": [], "source": ["# appropriately preprocess X"]}, {"cell_type": "code", "execution_count": 7, "metadata": {"index": 19}, "outputs": [], "source": ["# instantiate a pca object with 2 components and fit to preprocessed X"]}, {"cell_type": "code", "execution_count": 8, "metadata": {"index": 20}, "outputs": [], "source": ["# determine how much of the total variance is explained by the first two components"]}, {"cell_type": "code", "execution_count": 13, "metadata": {"index": 21}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["0.6324320765155941\n", "The first two principal components explain about 63% of the variance\n"]}], "source": ["from sklearn.preprocessing import StandardScaler\n", "from sklearn.decomposition import PCA\n", "\n", "# Scale the independent features\n", "ss = StandardScaler()\n", "ss.fit(X)\n", "\n", "X_sc = ss.transform(X)\n", "\n", "# instanstiate a pca object with 2 components\n", "pca = PCA(n_components=2)\n", "\n", "#fit to preprocessed X\n", "pca.fit(X_sc)\n", "\n", "print(sum(pca.explained_variance_ratio_))\n", "print('The first two principal components explain about 63% of the variance')"]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["<a id='nlp'></a>\n"]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["# NLP"]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["For NLP data, what is the entire data of records called?"]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["> your answer here"]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["What is an individual record called?"]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["> your answer here"]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["What is a group of two words that appear next to one-another in a document?"]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["> Your answer here"]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["What is a high frequency, semantically low value word called? "]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["> Your answer here"]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["List the preprocessing steps we can employ to create a cleaner feature set to our models."]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["> Your answer here"]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["Explain the difference between the two main vectorizors we employ to transform the data into the document-term matrix."]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["> Your answer here"]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["# NLP Code"]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": 37}, "outputs": [], "source": ["# data import\n", "policies = pd.read_csv('data/2020_policies_feb_24.csv')\n", "\n", "def warren_not_warren(label):\n", "    \n", "    '''Make label a binary between Elizabeth Warren\n", "    speeches and speeches from all other candidates'''\n", "    \n", "    if label =='warren':\n", "        return 1\n", "    else:\n", "        return 0\n", "    \n", "policies['candidate'] = policies['candidate'].apply(warren_not_warren)\n", "\n", "policies.head()"]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": 38}, "outputs": [], "source": ["# split into train and test set \n", "# note: for demonstration purposes, we will not use cross-validation here nor a holdout set."]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": 39}, "outputs": [], "source": ["# Import and instantiate a Count Vectorizer with defaults"]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": 40}, "outputs": [], "source": ["# Transform train and test sets with the Count Vectorizer\n", "# then fit a logistic regression model on it."]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": 41}, "outputs": [], "source": ["# Score on both train and test sets."]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": 42}, "outputs": [], "source": ["# Tune some hyperparameters of the vectorizer and assess the performance"]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["<a id='ts'></a>"]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["# Time Series"]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": 45}, "outputs": [], "source": ["import pandas as pd\n", "import numpy as np"]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": 46}, "outputs": [], "source": ["ap = pd.read_csv('data/AirPassengers.csv')"]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["With the data above, what is the first step in transforming it into data suitable for our time series models?"]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["> Your answer here"]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": 49}, "outputs": [], "source": ["# Perform that step in code"]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["What types of patterns might we expect to find in our time series datasets?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": 51}, "outputs": [], "source": ["# plot the time series"]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["What type of patterns do you see in the above plot?"]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["> Your answer here"]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": 54}, "outputs": [], "source": ["# Add to the plot to visualize patterns by looking at summary statistics across a window of time."]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["What are some ways to remove those trends? "]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["What is the goal of removing those trends?"]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["> Your answer here"]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": 58}, "outputs": [], "source": ["# Attempt to make the series stationary using differencing"]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["How can we diagnose whether we have successfully removed the trends?"]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["> Your answer here"]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["Use the Augmented Dickey Fuller test to see if the detrended data is ready for modeling"]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": 62}, "outputs": [], "source": ["# your code here"]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["<a id='clust'></a>"]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["# Clustering"]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["Question: What is the difference between supervised and unsupervised learning?"]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["> Your answer here"]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["Describe how the kmeans algorithm updates its cluster centers after initialization."]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["> Your answer here"]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["What is inertia, and how does kmeans use inertia to determine the best estimator?"]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["> Your answer here"]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": 71}, "outputs": [], "source": ["from sklearn.cluster import KMeans\n", "\n", "KMeans()"]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["What other metric do we have to score the clusters which are formed?"]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["Describe the difference between it and inertia."]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["> Your answer here"]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["# Code Cluster Practice with Heirarchical Agglomerative Clustering"]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["After the above conceptual review of KMeans, let's practice coding with agglomerative clustering."]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": 77}, "outputs": [], "source": ["from sklearn.datasets import load_iris\n", "\n", "data = load_iris()\n", "X = pd.DataFrame(data['data'])\n", "y = data['target']"]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": 78}, "outputs": [], "source": ["# Import the relevent clusterer and instantiate an instance of it. \n", "# Indicate the number of clusters you want"]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": 79}, "outputs": [], "source": ["# Preprocess the data"]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": 80}, "outputs": [], "source": ["# Fit the object"]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": 81}, "outputs": [], "source": ["# Calculate a silhouette score"]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": 82}, "outputs": [], "source": ["# Repeat with another choice for number of clusters"]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": 83}, "outputs": [], "source": ["# Determine which is a better number"]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["# Bonus: Use PCA to visualize in two dimensions the cluster groups of the best metric."]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": 85}, "outputs": [], "source": ["# your code here"]}], "metadata": {"kernelspec": {"display_name": "learn-env", "language": "python", "name": "learn-env"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.3"}}, "nbformat": 4, "nbformat_minor": 4}